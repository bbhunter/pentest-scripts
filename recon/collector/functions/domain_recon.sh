#############################################################
#                                                           #
# This file is an essential part of collector's execution!  #
# And is responsible to get the functions:                  #
#                                                           #
#   * subdomains_recon                                      #
#   * joining_removing_duplicates                           #
#   * managing_the_files                                    #
#   * hdc                                                   #
#                                                           #
#############################################################            

subdomains_recon(){
    if [ -d "${tmp_dir}" ]; then
        echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} We have ${#dns_servers[*]} executions for subdomains discovery and this might take a certain time!"
        for dns in "${!dns_servers[@]}"; do
            dns_position="${dns}"
            tput cup $((cursor_start_position -= 1)) 0
            echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} It's the $((dns_position += 1)) execution using the ${dns_servers[$((dns_position -= 1))]} DNS server!              "
            tput el
            # Backing the correct Cursor position
            $((cursor_start_position += 1))
            tput cup "${cursor_start_position}" 0
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing alienvault... "
            tput el
            #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s "https://otx.alienvault.com/api/v1/indicators/domain/${domain}/passive_dns" \
            curl "${curl_options[@]}" "https://otx.alienvault.com/api/v1/indicators/domain/${domain}/passive_dns" \
                | jq --raw-output '.passive_dns[]?.hostname' 2> ${log_dir}/recon_domain_error_${date_recon}.log | sort -u >> "${tmp_dir}/alienvault_output.txt"
            echo "Done!"
        
            tput cup "${cursor_start_position}" 0
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing amass... "
            tput el
            timeout $((${amass_timeout_execution} + 1))m ${amass_bin} enum -timeout "${amass_timeout_execution}"  -r "${dns_servers[${dns}]}" -d "${domain}" >> "${tmp_dir}/amass_output.txt" 2> "${tmp_dir}/error.log"
            timeout $((${amass_timeout_execution} + 1))m ${amass_bin} enum -timeout "${amass_timeout_execution}" -passive  -r "${dns_servers[${dns}]}" -d "${domain}" >> "${tmp_dir}/amass_passive_output.txt" 2> "${tmp_dir}/error.log"
            echo "Done!"

            if [[ -n ${binaryedge_api_url} ]] && [[ -n "${binaryedge_api_key}" ]]; then
                #binaryedge_api_check=$(curl -A "${curl_agent}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s -w %{http_code} "${binaryedge_api_url}/${domain}" -H "X-key: ${binaryedge_api_key}" -o /dev/null)
                binaryedge_api_check=$(curl "${curl_options[@]}" -w %{http_code} "${binaryedge_api_url}/${domain}" -H "X-key: ${binaryedge_api_key}" -o /dev/null)
                if [ "${binaryedge_api_check}" -eq 200 ]; then
                    tput cup "${cursor_start_position}" 0
                    echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing binaryedge.io... "
                    tput el
                    #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s "${binaryedge_api_url}/${domain}" \
                    curl "${curl_options[@]}" "${binaryedge_api_url}/${domain}" \
                        -H 'X-Key:'${binaryedge_api_key}'' | jq --raw-output -r '.events[]?' 2> /dev/null \
                        | sort -u >> "${tmp_dir}/binaryedge_output.txt"
                    echo "Done!"
                fi
            fi

            if [[ -n "${censys_api_url}" ]] && [[ -n "${censys_api_id}" ]] && [[ -n "${censys_api_secret}" ]]; then
                #censys_api_check=$(curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s -w %{http_code} -u "${censys_api_id}:${censys_api_secret}" -H 'Content-Type: application/json' -H 'Accept: application/json' -L -X POST "${censys_api_url}" --data-raw '{"query": "parsed.names: '${domain}'", "fields": ["parsed.names"]}' -o /dev/null 2> /dev/null)
                censys_api_check=$(curl "${curl_options[@]}" -s -w %{http_code} -u "${censys_api_id}:${censys_api_secret}" -H 'Content-Type: application/json' -H 'Accept: application/json' -L -X POST "${censys_api_url}" --data-raw '{"query": "parsed.names: '${domain}'", "fields": ["parsed.names"]}' -o /dev/null 2> /dev/null)
                if [ "${censys_api_check}" -eq 200 ]; then
                    tput cup "${cursor_start_position}" 0
                    echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing censys.io... "
                    tput el
                    #curl -A "${curl_agent}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s -u "${censys_api_id}:${censys_api_secret}"  -H 'Content-Type: application/json' \
                    curl "${curl_options[@]}" -u "${censys_api_id}:${censys_api_secret}"  -H 'Content-Type: application/json' \
                        -H 'Accept: application/json' -L -X POST "${censys_api_url}" --data-raw '{"query": "parsed.names: '${domain}'", "fields": ["parsed.names"]}' \
                        | jq '.results | .[] | .[]' 2> /dev/null | grep "${domain}" | sed -e '/,/d' -e 's/^[[:blank:]]*//' -e 's/"//g' | sort -u >> "${tmp_dir}/censys_output.txt"
                    echo "Done!"
                fi
            fi

            tput cup "${cursor_start_position}" 0
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing certspotter... "
            tput el
            #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s "https://api.certspotter.com/v1/issuances?domain=${domain}&include_subdomains=true&expand=dns_names" \
            curl "${curl_options[@]}" "https://api.certspotter.com/v1/issuances?domain=${domain}&include_subdomains=true&expand=dns_names" \
                | jq --raw-output -r '.[].dns_names[]' 2> /dev/null | sed 's/\"//g' | sed 's/\*\.//g' \
                | sort -u | grep "${domain}" >> "${tmp_dir}/certspotter_output.txt"

            echo "Done!"

            tput cup "${cursor_start_position}" 0
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing commoncrawl... "
            tput el
            commoncrawl_url="http://index.commoncrawl.org/collinfo.json"
            commoncrawl_status=$(curl "${curl_options[@]}" -w "%{http_code}\n" -s -o /dev/null "${commoncrawl_url}")
            commoncrawl_raw_file="${tmp_dir}/commoncrawl_raw_output.txt"
            commoncrawl_domains_file="${tmp_dir}/commoncrawl_domains_output.txt"
            
            if [[ ${commoncrawl_status} == 200 ]]; then
                #commoncrawl_db=$(curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s "${commoncrawl_url}" | jq --raw-output .[0]'."cdx-api"' 2> /dev/null)
                #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s "${commoncrawl_db}?url=*.${domain}/&output=json" | jq --raw-output .url? 2> /dev/null \
                commoncrawl_db=$(curl "${curl_options[@]}" -s "${commoncrawl_url}" | jq --raw-output .[0]'."cdx-api"' 2> /dev/null)
                curl "${curl_options[@]}" "${commoncrawl_db}?url=*.${domain}/&output=json" | jq --raw-output .url? 2> /dev/null \
                    | sed 's/\*\.//g' > "${commoncrawl_raw_file}"
                cat "${commoncrawl_raw_file}" | sed -e 's_https*://__' -e "s/\/.*//" -e 's/:.*//' -e 's/^www\.//' -e "/@/d" -e 's/\.$//' \
                    | sort -u >> "${commoncrawl_domains_file}"
            fi
            echo "Done!"

            tput cup "${cursor_start_position}" 0
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing crt.sh... "
            tput el
            #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s https://crt.sh/?q=%25."${domain}"\&output=json | jq -r '.[].name_value' 2> /dev/null | \
            curl "${curl_options[@]}" -s https://crt.sh/?q=%25."${domain}"\&output=json | jq -r '.[].name_value' 2> /dev/null | \
                sed 's/\*\.//g' | sort -u >> "${tmp_dir}/crtsh_output.txt"
            echo "Done!"

            if [[ -n "${dnsdb_api_url}" ]] && [[ -n "${dnsdb_api_key}" ]]; then
                #dnsdb_api_check=$(curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -gs -w "%{http_code}\n" -H "Accept: application/json" -H "X-API-Key: ${dnsdb_api_key}" "${dnsdb_api_url}/*.${domain}?limit=1000000000" -o /dev/null)
                dnsdb_api_check=$(curl "${curl_options[@]}" -g -w "%{http_code}\n" -H "Accept: application/json" -H "X-API-Key: ${dnsdb_api_key}" "${dnsdb_api_url}/*.${domain}?limit=1000000000" -o /dev/null)
                if [ "${dnsdb_api_check}"  -eq 200 ]; then
                    tput "${cursor_start_position}" 0
                    echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing dnsdb... "
                    tput el
                    #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -gs -H "Accept: application/json" -H "X-API-Key: ${dnsdb_api_key}" "${dnsdb_api_url}/*.${domain}?limit=1000000000" \
                    curl "${curl_options[@]}" -g -H "Accept: application/json" -H "X-API-Key: ${dnsdb_api_key}" \
                        "${dnsdb_api_url}/*.${domain}?limit=1000000000" \
                        | jq --raw-output -r .rrname? 2> /dev/null \
                        | sed -e 's/\.$//' \
                        | sort -u >> "${tmp_dir}/dnsdb_output.txt"
                    echo "Done!"
                fi
            fi

            tput cup "${cursor_start_position}" 0
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing dns dumpster... "
            tput el
            dnsdumpster_url="https://dnsdumpster.com"
            dnsdumpster_cookie=$(curl "${curl_options[@]}" -I "https://dnsdumpster.com"  | grep -E -i "^set-cookie:" | awk '{print $2}')
            dnsdumpster_csrf=$(curl "${curl_options[@]}" -L --cookie "${dnsdumpster_cookie}" "${dnsdumpster_url}" | grep -i -P  "csrfmiddlewaretoken" | grep -Po '(?<=value=")[^"]*(?=")')
            #dnsdumpster_csrf=$(curl "${curl_options[@]}" --cookie "${dnsdumpster_cookie}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s "${dnsdumpster_url}" | grep -P  "csrfmiddlewaretoken" | grep -Po '(?<=value=")[^"]*(?=")')
            #curl "${curl_options[@]}" --cookie "${dnsdumpster_cookie}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s --cookie "csrftoken=${dnsdumpster_csrf}" -H "Referer: ${dnsdumpster_url}" \
            curl "${curl_options[@]}" -L --cookie "${dnsdumpster_cookie}" -H "Referer: ${dnsdumpster_url}" \
                --data "csrfmiddlewaretoken=${dnsdumpster_csrf}&targetip=${domain}&user=free" ${dnsdumpster_url} \
                | grep -Po '<td class="col-md-4">\K[^<]*' \
                | sort -u \
                | grep "${domain}" >> "${tmp_dir}/dnsdumpster_output.txt"
            unset dnsdumpster_url
            unset dnsdumpster_cookie
            unset dnsdumpster_csrf
            echo "Done!"

            tput cup "${cursor_start_position}" 0
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing hackertarget... "
            tput el
            #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s --request GET "${hackertarget_url}${domain}" | awk -F',' '{print $1}' | sort -u >> "${tmp_dir}/hackertarget_output.txt"
            curl "${curl_options[@]}" -X GET "${hackertarget_url}${domain}" \
                | awk -F',' '{print $1}' \
                | sort -u \
                | grep -v "API count exceeded - Increase Quota with Membership" >> "${tmp_dir}/hackertarget_output.txt"
            echo "Done!"

        # netcraft
        # "https://searchdns.netcraft.com/?restriction=site+contains&host=${domain}&position=limited"

            tput cup "${cursor_start_position}" 0
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing rapiddns... "
            tput el
            #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s "https://rapiddns.io/subdomain/${domain}" | grep -Po 'target="_blank">\K[^<]*' | sort -u >> "${tmp_dir}/rapiddns_output.txt"
            curl "${curl_options[@]}" "https://rapiddns.io/subdomain/${domain}#result" \
                | grep "${domain}" \
                | sed 's/^<td>// ; s/<\/td>$// ; /^<input/d ; /^<meta/d ; /^<title>/d ; s/<\/a>// ; s/<a.*>// ; s/\.$//' \
                | sort -u >> "${tmp_dir}/rapiddns_output.txt"
            echo "Done!"

            if [[ -n "${riskiq_api_key}" ]] && [[ -n "${riskiq_api_secret}" ]]; then
                #riskiq_api_check=$(curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s -w "%{http_code}\n" -u "${riskiq_api_key}:${riskiq_api_secret}" "${riskiq_api_url}?query=${domain}" -o /dev/null)
                riskiq_api_check=$(curl "${curl_options[@]}" -w "%{http_code}\n" -u "${riskiq_api_key}:${riskiq_api_secret}" "${riskiq_api_url}?query=${domain}" -o /dev/null)
                if [ "${riskiq_api_check}" -eq 200 ]; then
                    tput cup "${cursor_start_position}" 0
                    echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing riskiq... "
                    tput el
                     #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s -u "${riskiq_api_key}:${riskiq_api_secret}" "${riskiq_api_url}?query=${domain}" \
                     curl "${curl_options[@]}" -s -u "${riskiq_api_key}:${riskiq_api_secret}" "${riskiq_api_url}?query=${domain}" \
                         | jq --raw-output -r .subdomains[]? 2> /dev/null | sort -u >> "${tmp_dir}/riskiq_output.txt"
                     sed -i "s/$/.${domain}/" "${tmp_dir}/riskiq_output.txt"
                     echo "Done!"
                fi
            fi

            if [[ -n "${securitytrails_api_key}" ]]; then
                #securitytrails_api_check=$(curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -w "%{http_code}\n" "${securitytrails_api_url}/${domain}/subdomains?apikey=${securitytrails_api_key}" -o /dev/null)
                securitytrails_api_check=$(curl "${curl_options[@]}" -w "%{http_code}\n" "${securitytrails_api_url}/${domain}/subdomains?apikey=${securitytrails_api_key}" -o /dev/null)
                if [ "${securitytrails_api_check}" -eq 200 ]; then
                    tput cup "${cursor_start_position}" 0
                    echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing security trails... "
                    tput el
                    #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" "${securitytrails_api_url}/${domain}/subdomains?apikey=${securitytrails_api_key}" \
                    curl "${curl_options[@]}" "${securitytrails_api_url}/${domain}/subdomains?apikey=${securitytrails_api_key}" \
                        | jq --raw-output -r '.subdomains[]' 2> /dev/null | sort -u >> "${tmp_dir}/securitytrails_output.txt"
                    sed -i "s/$/.${domain}/" "${tmp_dir}/securitytrails_output.txt"
                    echo "Done!"
                fi
            fi

            if [ "${shodan_use}" == "yes" ]; then
                tput cup "${cursor_start_position}" 0
                echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing shodan... "
                tput el
                "${shodan_bin}" search --no-color --fields hostnames hostname:"${domain}" 2> /dev/null | sed -e 's/;/\n/g' -e '/^$/d' | sort -u >> "${tmp_dir}/shodan_subdomain_output.txt"
                echo "Done!"
            fi

            tput cup "${cursor_start_position}" 0
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing subfinder... "
            tput el
            ${subfinder_bin} -silent -d "${domain}" >> "${tmp_dir}/subfinder_output.txt"
            echo "Done!"

            tput cup "${cursor_start_position}" 0
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing threatcrowd... "
            tput el
            #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s "https://threatcrowd.org/searchApi/v2/domain/report/?domain=${domain}" \
            curl "${curl_options[@]}" "https://threatcrowd.org/searchApi/v2/domain/report/?domain=${domain}" \
                | jq --raw-output -r '.subdomains[]?' 2> /dev/null \
                | grep -Eo "\b[A-Za-z0-9.-]+\.[A-Za-z]{2,6}\b" \
                | grep -P "${domain}" | sort -u >> "${tmp_dir}/threatcrowd_output.txt"
            echo "Done!"

            # Threatminer does not get brazilian domains
            tput cup "${cursor_start_position}" 0
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing threatminer... "
            tput el
            #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s "https://api.threatminer.org/v2/domain.php?q=${domain}&rt=5" \
            curl "${curl_options[@]}" "https://api.threatminer.org/v2/domain.php?q=${domain}&rt=5" \
                | jq --raw-output -r '.results[]?' 2> /dev/null | sort -u >> "${tmp_dir}/threatminer_output.txt"
            echo "Done!"

            if [[ -n "${virustotal_api_url}" ]] && [[ -n "${virustotal_api_key}" ]]; then
                #virustotal_api_check=$(curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -w "%{http_code}\n" "${virustotal_api_url}${virustotal_api_key}/&domain=${domain}" -o /dev/null)
                virustotal_api_check=$(curl "${curl_options[@]}" -w "%{http_code}\n" "${virustotal_api_url}${virustotal_api_key}/&domain=${domain}" -o /dev/null)
                if [ "${virustotal_api_check}" -eq 200 ]; then
                    tput cup "${cursor_start_position}" 0
                    echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing virus total... "
                    tput el
                    #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s "${virustotal_api_url}${virustotal_api_key}/&domain=${domain}" \
                    curl "${curl_options[@]}" "${virustotal_api_url}${virustotal_api_key}/&domain=${domain}" \
                        | jq --raw-output -r '.subdomains[]?' 2> /dev/null | sort -u >> "${tmp_dir}/virustotal_output.txt"
                    echo "Done!"
                fi
            fi

            tput cup "${cursor_start_position}" 0
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing webarchive... "
            tput el
            #curl "${curl_options[@]}" --dns-ipv4-addr "${dns_servers[${dns}]}" -s "http://web.archive.org/cdx/search/cdx?url=*.${domain}/*&output=text&fl=original&collapse=urlkey" \
            curl "${curl_options[@]}" "http://web.archive.org/cdx/search/cdx?url=*.${domain}/*&output=text&fl=original&collapse=urlkey" \
                | sed -e 's_https*://__' -e "s/\/.*//" -e 's/:.*//' -e 's/^www\.//' | sed "/@/d" | sed -e 's/\.$//' | sort -u >> "${tmp_dir}/webarchive_output.txt"
            echo "Done!"

            tput cup "${cursor_start_position}" 0
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Executing whoisxmlapi... "
            curl "${curl_options[@]}" -X POST "${whoisxmlapi_url}" \
                -H "Content-Type: application/json" \
                --data '{"apiKey": "'${whoisxmlapi_apikey}'", "domains": {"include": ["'${domain}'"]},"subdomains": {"include": [],"exclude": []}}' \
                | jq -r '.domainsList[]' >> "${tmp_dir}/whoisxmlapi_output.txt"
            echo "Done!"

            if [ "${#dns_wordlists[@]}" -gt 0 ]; then
                tput cup "${cursor_start_position}" 0
                echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} We will execute brute force dns with amass, gobuster and dnssearch ${#dns_wordlists[@]} time(s)."
                tput cup $((cursor_start_position += 1)) 0
                echo -e "\t Take a break as this step takes a while."

                for list in "${dns_wordlists[@]}"; do
                    index=$(printf "%s\n" "${dns_wordlists[@]}" | grep -En "^${list}$" | awk -F":" '{print $1}')
                    if [ -s "${list}" ]; then
                        tput cup $((cursor_start_position += 1)) 0
                        echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Execution number ${index}... "
                        tput el
                        ${amass_bin} enum -src -r "${dns_servers[${dns}]}" -w "${list}" -d "${domain}" >> "${tmp_dir}"/amass_brute_output_"${index}".txt 2> "${tmp_dir}/error.log"
                        ${gobuster_bin} dns -z -q -r "${dns_servers[${dns}]}" -t "${gobuster_threads}" -d "${domain}" -w "${list}" >> "${tmp_dir}"/gobuster_dns_output_"${index}".txt 2> "${tmp_dir}/error.log"
                        ${dnssearch_bin} -consumers 600 -domain "${domain}" -wordlist "${list}" | \
                        grep "${domain}" >> "${tmp_dir}"/dnssearch_output_"${index}".txt 2> "${tmp_dir}/error.log"
                        echo "Done!"
                    else
                        tput cup $((cursor_start_position += 1)) 0
                        echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Execution number ${index}, error: ${list} does not exist or is empty!"
                            continue
                    fi
                    unset index
                    cursor_start_position=30
                done
                unset list
                cursor_start_position=29
                echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Brute force execution of amass, gobuster and dnssearch is done."
            fi
    
            for ns in $(dig +short ns "${domain}" 2> /dev/null | sed -e 's/\.$//'); do
                if ! dig axfr "@${ns}" "${domain}" 2> /dev/null | grep -Ei "Transfer failed.|servers could be reached|timed out.|network unreachable.$" > /dev/null 2>&1; then
                    dig axfr "@${ns}" "${domain}" > "${tmp_dir}/zone_transfer.txt" 2> /dev/null
                fi
            done
        done
        tput cup $((cursor_start_position -=1 )) 0
        echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} The Subdomain Discovery is finished using ${#dns_servers[*]} DNS servers!"
        tput el
    else
        echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script!"
        exit 1
    fi
}

joining_removing_duplicates(){
    if [ -d "${tmp_dir}" ] && [ -d "${report_dir}" ]; then
        echo -en "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Putting all domain search results in one file... "

        if [ -s "${tmp_dir}/alienvault_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/alienvault_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/amass_output.txt" ]; then
            grep -Ev "Starting.*names|Querying.*|Average.*performed" "${tmp_dir}/amass_output.txt" | \
                grep "${domain}" | awk '{print $2}' | grep -E "^.*\.${domain}" | sort -u >> "${tmp_dir}/domains_found_tmp.txt"
            #grep -A 1000 "OWASP Amass.*OWASP/Amass" "${tmp_dir}/amass_output.txt" >> "${report_dir}/amass_blocks_output.txt"
            #grep "Subdomain Name(s)" "${report_dir}/amass_blocks_output.txt" | awk '{print $1}' | \
            #    grep -vE "^([0-9a-zA-Z]{1,4}:)" | sed '/0.0.0.0\/0/d' >> "${tmp_dir}/tmp_blocks.txt"
            #grep "Subdomain Name(s)" "${report_dir}/amass_blocks_output.txt" | awk '{print $1}' | \
            #    grep -E "^([0-9a-zA-Z]{1,4}:)" >> "${report_dir}/ipv6_blocks.txt"
        fi

        if [ -s "${tmp_dir}/amass_passive_output.txt" ]; then
            grep -Ev "Starting.*names|Querying.*|Average.*performed" "${tmp_dir}/amass_passive_output.txt" | \
                grep "${domain}" | awk '{print $2}' | grep -E "^.*\.${domain}" | sort -u >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/amass_intel.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/amass_intel.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi
        
        if [ -s "${tmp_dir}/binaryedge_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/binaryedge_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/censys_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/censys_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/certspotter_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/certspotter_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/commoncrawl_domains_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/commoncrawl_domains_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/crtsh_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/crtsh_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi
        
        if [ -s "${tmp_dir}/dnsbufferover_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/dnsbufferover_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/dnsdb_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/dnsdb_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/dnsdumpster_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/dnsdumpster_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/hackertarget_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/hackertarget_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/rapiddns_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/rapiddns_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/riskiq_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/riskiq_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/securitytrails_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/securitytrails_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/shodan_subdomains_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/shodan_subdomains_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/subfinder_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/subfinder_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/threatcrowd_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/threatcrowd_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/threatminer_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/threatminer_output.txt" >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/webarchive_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/webarchive_output.txt" 2> /dev/null >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ -s "${tmp_dir}/whoisxmlapi_output.txt" ]; then
            grep -E "^.*\.${domain}" "${tmp_dir}/whoisxmlapi_output.txt" 2> /dev/null >> "${tmp_dir}/domains_found_tmp.txt"
        fi

        if [ ${#dns_wordlists[@]} -gt 0 ]; then
            files_amass=($(ls -1A "${tmp_dir}/" | grep "amass_brute_output" 2> /dev/null))
            for f in "${files_amass[@]}"; do
                file="${tmp_dir}"/"${f}"
                if [[ -s "${file}" ]]; then
                    grep -Ev "Starting.*names|Querying.*|Average.*performed" "${file}" \
                        | grep "${domain}" | awk '{print $2}' | grep -E "^.*\.${domain}" \
                        | sort -u >> "${tmp_dir}/domains_found_tmp.txt"
                fi
                unset file
            done

            files_gobuster_dns=($(ls -1A "${tmp_dir}/" | grep "gobuster_dns_output" 2> /dev/null))
            for f in "${files_gobuster_dns[@]}"; do
                file="${tmp_dir}"/"${f}"
                if [[ -s "${file}" ]]; then
                    awk '{print $2}' "${file}" | tr '[:upper:]' '[:lower:]' \
                        | grep -E "^.*\.${domain}" | sort -u >> "${tmp_dir}/domains_found_tmp.txt"
                fi
                unset file
            done

            files_dnssearch=($(ls -1A "${tmp_dir}/" | grep "dnssearch_output_" 2> /dev/null))
            for f in "${files_dnssearch[@]}"; do
                file="${tmp_dir}"/"${f}"
                if [[ -s "${file}" ]]; then
                    awk '{print $1}' "${file}" | tr '[:upper:]' '[:lower:]' \
                        | grep -E "^.*\.${domain}" | sort -u >> "${tmp_dir}/domains_found_tmp.txt"
                fi
                unset file
            done
        fi
        echo "Done!"
        
        if [ -s "${tmp_dir}/domains_found_tmp.txt" ]; then
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Joining the subdomains and removing duplicates... "
            # Removing duplicated subdomains
            cp "${tmp_dir}/domains_found_tmp.txt" "${tmp_dir}/domains_found_tmp.old"
            sed -E -i 's/^@//g ; s/^\.//g ; s/^-//g ; s/^\://g ; s/\.\./\./g ; s/^http(|s):\/\///g ; s/ //g ; s/^$//g ; /^[[:space:]]*$/d' "${tmp_dir}/domains_found_tmp.txt"
            # Removing duplicated domains per subdomain
            # Example: www.domain.com.domain.com.domain.com
            sed -i "s/\.${domain}//g" "${tmp_dir}/domains_found_tmp.txt"
            sed -i "s/\.$//g" "${tmp_dir}/domains_found_tmp.txt"
            sed -i "s/$/\.${domain}/g" "${tmp_dir}/domains_found_tmp.txt"

            if tr '[:upper:]' '[:lower:]' < "${tmp_dir}/domains_found_tmp.txt" | sort -u > "${report_dir}/domains_found.txt" ; then
                sed -i '/owasp.*nonce/d ; /_domainkey/d ; /^[[:blank:]]/d ; /</d ; />/d' "${report_dir}/domains_found.txt"
                echo "Done!"
            fi

            if [ ${#excluded[@]} -gt 0 ] && [ -s "${report_dir}/domains_found.txt" ]; then
                echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Excluding the subdomains from command line option... "
                for subdomain in "${excluded[@]}"; do
                    sed -i "/^${subdomain}$/d" "${report_dir}/domains_found.txt"
                done
                unset subdomain
                # Fixing blank lines after excluding domains
                sed -i '/^$/d' "${report_dir}/domains_found.txt"
                echo "Done!"
            fi

            if [ -s "${exclude_domains_list}" ] && [ -s "${report_dir}/domains_found.txt" ]; then
                echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Excluding the subdomains from file list option... "
                cp "${exclude_domains_list}" "${report_dir}/domains_excluded.txt"
                while read -r excluded_domain; do
                    sed -i "s/${excluded_domain}//" "${report_dir}/domains_found.txt"
                done < "${exclude_domains_list}"
                # Fixing blank lines after excluding domains
                sed -i '/^$/d' "${report_dir}/domains_found.txt"
                echo "Done!"
            fi
        fi

        echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Looking for Zone Transfer... "
        if [ -s "${tmp_dir}/zone_transfer.txt" ]; then
            cp "${tmp_dir}/zone_transfer.txt" "${report_dir}/zone_transfer.txt"
            echo "Done!"
        else
            echo "Fail!"
            echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Does not possible perfom zone transfer!"
        fi

    else
        echo "Fail!"
        echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
        exit 1
    fi
}

managing_the_files(){
    subdomains_file="$1"
    if [ -s "${subdomains_file}" ]; then
        echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Getting the IPs and aliases of the domain and subdomains... "
        # Domains and subdomains resolution
        "${massdns_bin}" -q -r "${dns_resolvers_file}" -t A -o S \
            -w "${tmp_dir}/domains_massdns_resolution.txt" "${subdomains_file}" > /dev/null 2>&1

        for dns in "${dns_servers[@]}"; do
            for d in $(cat "${subdomains_file}"); do
                dig +nocmd +nocomments +noquestion +noqr +nostats +timeout=2 -t A "${d}" "@${dns}" >> "${tmp_dir}/domains_dig_command_resolution.txt"
            done
        done

        for d in $(cat "${subdomains_file}"); do
            host -W 2 -t A "${d}" >> "${tmp_dir}/domains_host_command_resolution.txt"
        done

        # Organizing and handling domain files
        for file_resolution in "${tmp_dir}/domains_massdns_resolution.txt" "${tmp_dir}/domains_dig_command_resolution.txt" "${tmp_dir}/domains_host_command_resolution.txt"; do
            cp "${file_resolution}" "${file_resolution}.old"
            if [[ -s "${file_resolution}" ]];  then
                sed -i "s/${domain}\./${domain}/g" "${file_resolution}"
                sed -i "s/\.$//g" "${file_resolution}"
                sed -i 's/\.[[:blank:]]/ /g' "${file_resolution}"
                sed -i "s/^@//g" "${file_resolution}"
                sed -i "s/^\.//g" "${file_resolution}"
 
                # Domains with IPs
                grep -E "${IPv4_regex}$" "${file_resolution}" \
                    | grep "${domain}" | sort -u | awk '{print $1"\t"$NF}' >> "${report_dir}/domains_external_ipv4.txt"
                #grep -E "${IPv6_regex}$" "${tmp_dir}/domains_massdns_resolution_ipv6.txt" \
                #    | grep "${domain}" | sort -u | awk '{print $1"\t"$3}' >> "${report_dir}/domains_external_ipv6.txt"

                # Domains aliases
                grep -E "CNAME|is.an.alias" "${file_resolution}" | grep "${domain}" | sort -u | awk '{print $1"\t"$NF}' >> "${report_dir}/domains_aliases.txt"
            fi
        done
        echo "Done!"

        # Removing private IPs
        echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Separating internal and external IPs... "
        if [ -s "${report_dir}/domains_external_ipv4.txt" ]; then
            grep -E '(\b10\.\b([01]?[0-9][0-9]?|2[0-4][0-9]|25[0-5])\..*|^\S+\s+(127\..*)\b|172\.1[6789]\..*|172\.2[0-9]\..*|172\.3[01]\..*|192.168\..*)'$ "${report_dir}/domains_external_ipv4.txt" >> "${report_dir}/domains_internal_ipv4.txt"
            sed -i -E '/\b10\.\b([01]?[0-9][0-9]?|2[0-4][0-9]|25[0-5])\..*$/d ; /^\S+\s+(127\..*)\b$/d; /172\.1[6789]\..*$/d ; /172\.2[0-9]\..*$/d ; /172\.3[01]\..*$/d ; /192\.168\..*$/d' "${report_dir}/domains_external_ipv4.txt"
            cat "${report_dir}/domains_external_ipv4.txt" | awk '{print $1}' | sort -u >> "${tmp_dir}/domains_alive_tmp.txt"
        fi
        echo "Done!"

        # Getting sudomain aliases
        echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Separating subdomain aliases... "
        if [ -s "${report_dir}/domains_aliases.txt" ]; then
            cat "${report_dir}/domains_aliases.txt" | awk '{print $1}' | sort -u >> "${tmp_dir}/domains_alive_tmp.txt"
        fi
        echo "Done!"

        if sort -u -o "${report_dir}/domains_alive.txt" "${tmp_dir}/domains_alive_tmp.txt"; then
            # Unavailable domains
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Separating live subdomains from unresponsive subdomains... "
            cp "${subdomains_file}" "${report_dir}/domains_without_resolution.txt"
            for d in $(cat "${report_dir}/domains_alive.txt"); do
                sed -i "/${d}/d" "${report_dir}/domains_without_resolution.txt"
            done
            echo "Done!"

            # Sorting out...
            echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Removing duplicate subdomains... "
            sort -u -o "${report_dir}/domains_aliases.txt" "${report_dir}/domains_aliases.txt" 2> /dev/null
            sort -u -o "${report_dir}/domains_alive.txt" "${report_dir}/domains_alive.txt" 2> /dev/null
            sort -u -o "${report_dir}/domains_internal_ipv4.txt" "${tmp_dir}/domains_found_tmp_internal_ips.txt" 2> /dev/null
            sort -u -o "${report_dir}/domains_external_ipv4.txt" "${report_dir}/domains_external_ipv4.txt" 2> /dev/null
            #sort -u -o "${report_dir}/domains_external_ipv6.txt" "${report_dir}/domains_external_ipv6.txt" 2> /dev/null
            sort -u -o "${report_dir}/domains_without_resolution.txt" "${report_dir}/domains_without_resolution.txt" 2> /dev/null
            echo "Done!"
        else
            echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>> Error organizing and handling subdomain files!${reset}"
            echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Could not find any live domains, exiting!"
            exit 1
        fi

    else
        echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} The file with all domains from initial recon does not exist or is empty."
        echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Look all files from initial recon in ${tmp_dir} and fix the problem!"
        exit 1
    fi    
}

hdc(){
    # Horizontal Domain Correlation
    # Find all possible domains
    echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Looking for Horizontal Domain Correlation... "
    emails=($(whois "${domain}" | grep -E "Registrant Email|^e-mail" | grep -E -ho "[[:graph:]]+@[[:graph:]]+"))
    owner_id=($(whois "${domain}" | grep -Ei "^ownerid:|^Registry.Domain.ID:" | awk -F':' '{print $2}' | sed 's/[[:blank:]]//'))
    domains_found_temp+=("${domain}")

    if [ "$(echo "${domain}" | awk -F "." '{print $NF}')" == "br" ]; then
        brazilian="yes"
        echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} It's a Brazilian domain, so we can see how much domains the ${owner_id} has."
        xsrf_token_value="$(curl -I -kLs -A "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36" "https://registro.br/v2/ajax/payment/policy/com.br" | grep -Ei "^set-cookie: " | awk '{print $2}' | sed 's/;$// ; s/^XSRF.*=//')"
        number_of_domains=$(curl -H "Cookie: XSRF=${xsrf_token_value}" -H "X-Xsrf-Token: ${xsrf_token_value}" -kLs -A "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36" "https://registro.br/v2/ajax/whois/?qr=${owner_id}" | jq ".[]" | grep DomainCount | awk -F':' '{print $2}' | sed 's/,$// ; s/^[[:blank:]]//')
    fi
    echo "Done!"

    if [[ -n "${brazilian}" && "${brazilian}" == "yes" && ${number_of_domains} -gt 0 ]]; then
        echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} We have around ${number_of_domains} brazilian domain(s) to find!"
    fi

    echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Searching domains... "

    for email in "${emails[@]}"; do
        # Domain Eye is a paid service, if you have an access
        # and would like to collaborate please PR
        # https://domaineye.com/reverse-whois/

        domains_reversewhois+=($(curl -kLs "https://www.reversewhois.io/?searchterm=${email}" | \
            "${html2text_bin}" | grep -E "^[0-9]"| awk '{print $2}' | sed 's/|//'))
        domains_found_temp+=("${domains_reversewhois[@]}")

        domains_viewdns+=($(curl -kLs -A "${curl_agent}" "https://viewdns.info/reversewhois/?q=${email}" | "${html2text_bin}" | \
            grep -Po "[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)" | \
            grep -Evi "viewdns|${email}|favicon\.ico|cloudflare.com"))
        domains_found_temp+=("${domains_viewdns[@]}")
    done

    for d in $(printf "%s\n" ${domains_found_temp[@]} | sort -u); do
        domains_found+=("${d}")
    done
    unset d

    if [ ${#domains_found[@]} -gt 0 ]; then
        echo "Done!"
        domains_horizontal+=($(echo ${domains_found[@]} | tr ' ' '\n' | sort -u))
        printf "%s\n" "${domains_horizontal[@]}" >> "${report_dir}/domains_correlation_found.txt"
        echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} We found ${#domains_found[@]} domain(s)."

        # Finding valid domain (that have valid A record).
        # After that domains without A record will be rechecked.

        echo -ne "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} Domains validation (DNS requests)... "

        for d in ${domains_horizontal[@]}; do 
            ipv4=($(dig "${d}" A +short +time=5 +tries=3 @"${dns_servers[0]}"))
            ipv6=($(dig "${d}" AAAA +short +time=5 +tries=3 @"${dns_servers[0]}" | grep -E "${IPv6_regex}"))
            if [ -n "${ipv4}" ]; then
                echo -e "${d}\t${ipv4}" >> "${report_dir}/domains_correlation_ipv4.txt"
            elif [ -n "${ipv6}" ]; then
                echo -e "${d}\t${ipv6}" >> "${report_dir}/domains_correlation_ipv6.txt"
            fi
        done
        sed -i "/^${domain}/d" "${report_dir}/domains_correlation_found.txt" \
                               "${report_dir}/domains_correlation_ipv4.txt" \
                               "${report_dir}/domains_correlation_ipv6.txt" 2> /dev/null
        echo "Done!"
    else
        echo "Fail!"
        echo -e "${yellow}$(date +"%d/%m/%Y %H:%M")${reset} ${red}>>${reset} No domains correlation founds!"
    fi
}
